---
title             : "Modeling Memory: Exploring the Relationship Between Word Overlap and Single Word Norms when Predicting Relatedness Judgments and Retrieval"

shorttitle        : "Judgments and Recall"

author: 
  - name          : "Nicholas P. Maxwell"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "901 S. National Ave, Springfield, MO, 65897"
    email         : "maxwell270@live.missouristate.edu"
  - name          : "Erin M. Buchanan"
    affiliation   : "1"

affiliation:
  - id            : "1"
    institution   : "Missouri State University"


author_note: >
  Nicholas P. Maxwell is a graduate student at Missouri State University. Erin M. Buchanan is an Associate Professor of Psychology at Missouri State University.

abstract: >
 Need new abstract explaining results
  

  
keywords          : "judgments, memory, association, semantics, thematics"

bibliography      : ["nick_ref2.bib"]

figsintext        : no
figurelist        : no
tablelist         : no
footnotelist      : no
lineno            : yes

lang              : "english"
class             : "man"
output            : papaja::apa6_pdf
---

```{r libraries, include = FALSE}
knitr::opts_chunk$set(cache = TRUE)
library(papaja)
library(nlme)
library(lme4)
library(cowplot)
library(MOTE)
library(ggplot2)
library(MASS)
library(relaimpo)

##cleanup code for graphs
cleanup = theme(panel.grid.major = element_blank(), 
                panel.grid.minor = element_blank(), 
                panel.background = element_blank(), 
                axis.line.x = element_line(color = "black"),
                axis.line.y = element_line(color = "black"),
                legend.key = element_rect(fill = "white"),
                text = element_text(size = 15))

p.value = function(x){
  if (x < .001) { return("< .001")}
  else { return(apa(x, 3, F))}
}

```
  
  Previous research conducted on Judgments of Associative Memory (JAM) has found that these judgments tend to be stable and highly generalizable across varying contexts [@Maki2007a; @Maki2007; @Valentine2013]. The JAM task can be viewed as a manipulation of the traditional Judgment of Learning task (JOL). In a JOL task, participants are presented with cue-target word pairs and are asked to make a judgment (typically, on a scale of zero to one hundred) of how accurately they would be able to respond with the proper target word based on the presentation of a particular cue word [@Nelson1991; @Dunlosky1994a]. JAM tasks expand upon this concept by changing the focus of the judgments performed by participants. When presented with the item pair, such as *cheese-mouse*, participants are asked to judge the number of people out of 100 who would respond with the pair's target word (*mouse*) if they were only shown the cue word [*cheese*; @Maki2007a].

  This process mimics the creation of associative words norms [i.e., forward strength; @Nelson2004]. As such, these judgments can be viewed as the participants' approximations of how associatively related they perceive the paired items to be. The JAM function can then be created by plotting participants' judgments against the word's normed associative strength and calculating a line of best fit. This fit line typically displays a high intercept (bias) and a shallow slope (sensitivity), meaning that participants are biased towards overestimating the associative relatedness between word pairs, and show difficulties differentiating between different amounts of item relatedness [@Maki2007a]. These results are often found in JOL research [@Koriat2005], and they are highly stable across various contexts and instructional manipulations in JAM tasks [@Valentine2013].
  
  Building upon this research, we initially explored recall accuracy within the context of word pair judgments, while also expanding the JAM task to incorporate judgments of semantic and thematic memory. In the pilot study, 63 word-pairs of varying associative, semantic, and thematic overlap were created and arranged into three blocks, consisting of 21 word-pairs each [@Maxwell2018]. Associative overlap was measured with forward strength [FSG; @Nelson2004], semantic overlap was measured with cosine [COS; @Buchanan2013; @McRae2005; @Vinson2008], and thematic relatedness between pairs was measured with latent semantic analysis [LSA; @Landauer1997; @Landauer1998]. These word pairs were then judged by participants who were recruited from Amazon's Mechanical Turk. Stimuli were arranged into three blocks based, each preceeded by a set of instructions explaining either an associative, semantic, or thematic relationship between words. Three versions of the study were created, counterbalancing the order in which judgment instructions and stimuli blocks appeared. Thus, each participant made one set of judgments corresponding to each type of memory, and each word pair recieved each type of judgment. 
  
  After completing the judgment phase, participants then completed a cued recall task in which they were presented with the cue word from each of the previously presented word pairs and were asked to complete each pair with the missing target. Significant three-way interactions were found between database norms when predicting judgments and recall. When semantic overlap was low, thematic and associative strength were competitive, with increases in thematic overlap decreasing the strength of associative overlap as a predictor. However, this trend saw a reversal when semantic overlap was high, with thematic and associative strength complimenting one another. Overall, our findings from this study indicated the degree to which the processing of associative, semantic, and thematic information impacts retrieval and judgment making, while also displaying the interactive relationship that exists between these three types of information.
  
  This study seeks to expand upon this work by extending the original analysis to include multiple single word norms. These norms provide information about different "neighborhoods" of concept information. Broadly speaking, they can be separated into one of three categories. Base values refer to norms which capture information based on a word's structure. These include part of speech, word frequency, and the number of syllables, morphemes, and phonemes that comprise a word. Rated values refer to age of acquisition, concreteness, imageability, valence, and familiarity. Finally, we seek to examine norms that provide information about the connections or neighborhood a concept shares with others based on context. These norms include orthographic neighborhood, phonographic neighborhood, cue and target set sizes, and feature set size. These values were selected on the basis of previous research suggesting their impact on retrieval accurarcy; their importance is elaborated upon below. 

  First, we are interested in assessing the impact of base word norms. Chief amongst these is word frequency. Several sets of norms currently exist for measuring the frequency with which words occur in everyday language, and it is important to determine which of these offers the best representation of everyday language. One of the most commonly used collections of these norms is the @Kucera1967 frequency norms. This set consists of frequency values for words, which were generated by analyzing books, magazines, and newspapers. However, the validity of using these norms has been questioned on factors such as the properties of the sources analyzed, the size of the corpus analyzed, and the overall age of these norms. First, these norms were created from an analysis of written text. It is important to keep in mind that stylistically, writing tends to be more formal than everyday language and as a result, it may not be the best approximation of it [@Brysbaert2009]. Additionally, these norms were generated fifty years ago, meaning that these norms may not accurately reflect the current state of the English language. As such, the @Kucera1967 norms, while popular, may not be the best choice for researchers interested in gauging the effects of word frequency.

  Several viable alternatives to the @Kucera1967 frequency norms now exist. One popular method is to use frequency norms obtained from the HAL corpus, which consists of 131 million words [@Burgess1997; @Lund1996]. Other collections of frequency norms include CELEX [@Baayen1995] based on written text, the Zeno frequency norms [@Zeno1995] created from American children's textbooks, and Google Book's collection of word frequencies derived from 131 billion words taken from books published in the United States (see @Brysbaert2011 for an overview and comparison of these norms). For the present study, we plan to use data taken from the both the SUBTLEX project [@Brysbaert2009], which is a collection of frequency norms derived from a corpus of approximately 51 million words, which were generated from movie and television subtitles and the HAL corpus. SUBTLEX norms are thought to better approximate everyday language, as lines from movies and television tend to be more reflective of everyday speech than writing samples. Additionally, the larger corpus size of both SUBTLEX and HAL contributes to the validity of these norms compared to @Kucera1967 frequency norms.
  
  Next, we are interested in testing the effects of several measures of lexical information related to the physical make-up of words. These measures include the numbers of phonemes, morphemes, and syllables that comprise each word as well as its part of speech. The number of phonemes refers to the number of individual sounds that comprise a word (i.e., the word *cat* has three phonemes, each of which correspond to the sounds its letters make), while the term morpheme refers to the number of sound units that contain meaning. *Drive* contains one morpheme, while *driver* contains two. Morphemes typically consist of root words and their affixes. Additionally, word length (measured as the number of individual characters a word consists of) and the number of syllables a word contains will be investigated, as previous research has suggested that the number of syllables may play a role in processing time. In general, longer words require longer processing time [@Kuperman2012], and shorter words tend to be more easily remembered [@Cowan2003]. Finally, we are interested in the part of speech of each word, as nouns are often easier to remember [@Paivio1971]. Formally defined, part of speech refers to a word's categorization in language based on its syntactic functions.

  Third, we will examine the effects of norms measuring word properties that are rated by participants. The first of these is age of acquisition, which is a measure of the age at which a word is learned. This norm is measured by presenting participants with a word and having them enter the age (in years) in which they believe that they would have learned the word [@Kuperman2012]. Age of acquisition ratings have been found to be predictive of recall; for example, @Dewhurst1998 found recall to be higher for late acquired words. Also of interest are measures of a word's valence, which refers to its intrinsic pleasantness or perceived positiveness [@Bradley1999]. Valence ratings are important across multiple psycholinguistic research settings. These include research on emotion, the impact of emotion of lexical processing and memory, estimating the sentiments of larger passages of text, and estimating the emotional value of new words based on valence ratings of semantically similar words (see @Warriner2013 for a review). The next of these rated measures is concreteness, which refers to the degree that a word relates to a perceptible object [@Brysbaert2013]. Similar to concreteness, imageability is described as being a measure of a word's ability to generate a mental image [@Stadthagen-Gonzalez2006]. Both imageability and concreteness have been linked to recall, as items rated higher in these areas tend to be more easily recalled [@Nelson1992]. Finally, familiarity norms can be described as an application of word frequency. These norms measure the frequency of exposure to a particular word [@Stadthagen-Gonzalez2006].

  The final group of norms that will be investigated are those which provide information based on connections with neighboring words. Phonographic neighborhood refers to the number of words that can be created by changing one sound in a word (i.e., *cat* to *kite*). Similarly, orthographic neighborhood refers to the number of words created by changing a single letter in word [i.e., *cat* to *bat*, @Adelman2007; @Peereman1997]. Previous findings have suggested that the frequency of a target word relative to that of its orthographic neighbors has an effect on recall, increasing the likelihood of recall for that word [@Carreiras1997]. Additionally, both of measures have been found to effect processing speed for items [@Adelman2007; @Coltheart1977]. Next, we are interested in examining two single word norms that are directly related to item associations. These norms measure the number of associates a word shares connections with. Cue set size refers to the number of cue words that a target word is connected to, while target set size is a count of the number of target words a cue word is connected to [@Schreiber1998]. Previous research has shown evidence for a cue set size effect in which cue words that are linked to a larger number of associates (target words) are less likely to be recalled than cue words linked to fewer target words [@Nelson1999]. As such, feature list and cosine set sizes will be calculated for each word overlap norm from the @Buchanan2013 semantic feature norm set.

  In summary, this study seeks to expand upon previous work by examining how single word norms belonging to these three neighborhoods of item information impact the accuracy of item judgments and recall. These findings will be assessed within the context of associative, semantic, and thematic memory systems. Specifically, we utilize a three-tiered view of the interconnections between these systems as it relates to processing concept information. First, semantic information is processed, which provides a means for categorizing concepts based on feature similarity. Next, processing moves into the associative memory network, where contextual information pertaining to the items is added. Finally, the thematic network incorporates information from both the associative and semantic networks to generate a mental representation of the concept containing both the items meaning and its place in the world.

  Therefore, the present study has two aims. First, we seek to replicate the interaction results from the previous study on a conceptual level using a new set of stimuli. Second, we wish to expand upon these findings by extending the analysis to include single word information for the item pairs. This analysis was conducted by introducing the different types single word norms through a series of steps based on the type of neighborhood they belong to. First, base word norms were analyzed. Next, measures of word ratings were analyzed. Third, single word norms measuring connections between concepts were analyzed. Finally, network norms and their interactions were reanalyzed. The end goal was to determine both which neighborhood of norms have the greatest overall impact on recall and judgment ability, and to further assess the impact of network connections after controlling for the various neighborhoods of single word information.

# Method

## Participants

  First, a power analysis was conducted using the simr package in *R* [@Green2016]. This package uses simulations to calculate power for mixed linear models created from the lme4 and nlme packages in *R* [@Bates2015; @Pinheiro2017]. The results of this analysis suggested a minimum of 35 participants would be required to detect an effect at 80% power. However, because power often tends to be underestimated, participant recruitment was extended within the confines of available funding [@Brysbaert2018]. Thus, 221 participents were recruited from Amazon's Mechanical Turk, which is a website that allows individuals to host projects and connects them with a large pool of respondents who complete them for small amounts of money [@Buhrmester2011]. Participants who completed the study correctly were compensated $2.00 for their participation (*n* = 200). Participant responses were screened for a basic understanding of the study's instructions to justify payment and use in the study. Common reasons for rejecting responses included participants entering related words when numerical judgment responses were required, responding with numerical ratings during the cued recall task, or participants responding to the cue words during recall with phrases or sentences instead of individual words.

PARAGRAPH HERE ABOUT MAKING THE COMBINED DATASET

## Materials

  First, keeping consistent with the design of the original pilot study, sixty-three word pairs of varying associative, semantic, and thematic overlap were created to be used as stimuli. As with the pilot study, these word pairs were created using the @Buchanan2018a word norm database, which is an extension of the original @Buchanan2013 dataset. Next, neighborhood information for all cue and target items was collected. Word frequency was collected from the SUBTLEX project [@Brysbaert2009]. Part of speech, word length, and the number of morphemes, phonemes, and syllables of each item was derived from the @Buchanan2013 word norms [originally contained in The English Lexicon Project, @Balota2007]. For items with multiple parts of speech (for example, *drink* can refer to both a beverage and the act of drinking a beverage), part of speech was coded as the most commonly used form. Following the design of @Buchanan2013, this part of speech was determined using Google's define feature. Concreteness, cue set size, and target set size were taken from the South Florida Free Association Norms [@Nelson2004]. Feature set size (i.e., the number of features listed as part of the definition of a concept) and cosine set size (i.e., number of semantically related words above a cosine of zero) were calculated from @Buchanan2018a. Imageability and familiarity norms were taken from the Toglia and colleagues set of semantic word norms [@Toglia2009; @Toglia1978]. Age of acquisition ratings were pulled from the @Kuperman2012 database. Finally, valence ratings for all items were obtained from the @Warriner2013 norms. Stimuli information for cue and target words can be found in Tables \@ref(tab:stim-table-cue) and \@ref(tab:stim-table-target).
  
  After gathering neighborhood information, network norms measuring associative, semantic, and thematic overlap were generated for each pair. Forward strength (FSG) was used as a measure of associative overlap. FSG is a value ranging from zero to one which measures of the probability that a cue word will elicit a particular target word in response to it [@Nelson2004]. Cosine (COS) strength was used to measure semantic overlap between concepts [@Buchanan2018a; @McRae2005; @Vinson2008]. As with FSG, this value ranges from zero to one, with higher values indicating more shared features between concepts. Finally, thematic overlap was measured with Latent Semantic Analysis (LSA), which is a measure generated based upon the co-occurrences of words within a document [@Landauer1997; @Landauer1998]. Like the measures of associative and semantic overlap, LSA values range from zero to one, with higher values indicating higher co-occurrence between items. The selected stimuli contained a range of values across both the network and neighborhood norms. As with the previous study, stimuli will be arranged into three blocks, with each block consisting of 21 word pairs. The blocks will be structured to have seven words of low COS (0 - .33), medium COS (.34 - .66), and high COS (.67 - 1). COS was chosen due to both limitations with the size of the available data set across all norm sets, and the desire to recreate the selection process used for the previous study. The result of this selection process is that values for the remaining network norms (FSG and LSA) and information neighborhood norms will be contingent upon the COS strengths of the selected stimuli. To counter this, we selected stimuli at random based on the different COS groupings so as to cover a broad range of FSG, LSA, and information neighborhood values. Stimuli information for word pair norms can be found in Table \@ref(tab:Stimuli-Table). Table \@ref(tab:cue-table) displays stimuli averages, SD, and ranges of single word norms across cue items, while Table \@ref(tab:target-table) diplays this information for target items.  All stimuli and their raw values can be found at https://osf.io/j7qtc/.

```{r Stimuli-Table, results = 'asis', echo=FALSE}
####stimuli table####
tableprint = matrix(NA, nrow = 8, ncol = 10)
colnames(tableprint) = c("Variable", " ", "COS Low", " ",
                         " ", "COS Average", " ", 
                         " ", "COS High", " ")
library(MOTE)

stimuli = read.csv("stimuli jan 18.csv")

##calculate means and sds
##apa function is apa(numbers, decimals, leading zero T or F)
cosmean = apa(tapply(stimuli$COS, stimuli$cosg, mean), 3, F)
fsgmean = matrix(apa(
  tapply(stimuli$FSG, list("cos" = stimuli$cosg, "fsg" = stimuli$fsgg), mean), 3, F), 
  3, 3)
lsamean = matrix(apa(
  tapply(stimuli$LSA, list("cos" = stimuli$cosg, "lsa" = stimuli$lsag), mean), 3, F),
  3,3)

cossd = apa(tapply(stimuli$COS, stimuli$cosg, sd), 3, F)
fsgsd = matrix(apa(
  tapply(stimuli$FSG, list("cos" = stimuli$cosg, "fsg" = stimuli$fsgg), sd), 3, F),
  3,3)
lsasd = matrix(apa(
  tapply(stimuli$LSA, list("cos" = stimuli$cosg, "lsa" = stimuli$lsag), sd), 3, F),
  3,3)

cosN = tapply(stimuli$COS, stimuli$cosg, length)
fsgN = tapply(stimuli$FSG, list("cos" = stimuli$cosg, "fsg" = stimuli$fsgg), length)
lsaN = tapply(stimuli$LSA, list("cos" = stimuli$cosg, "lsa" = stimuli$lsag), length)

tableprint[1, ] = c(" ", "$N$", "$M$", "$SD$", "$N$", "$M$", "$SD$", "$N$", "$M$", "$SD$")
tableprint[2, ] = c("COS", cosN[2], cosmean[2], cossd[2],
                    cosN[3], cosmean[3], cossd[3],
                    cosN[1], cosmean[1], cossd[1])



##apply creates rows cos h l m
##apply creates columns fsg h l m

tableprint[3, ] = c("FSG Low", fsgN[2,2], fsgmean[2,2], fsgsd[2,2],
                    fsgN[3,2], fsgmean[3,2], fsgsd[3,2],
                    fsgN[1,2], fsgmean[1,2], fsgsd[1,2])
tableprint[4, ] = c("FSG Average", fsgN[2,3], fsgmean[2,3], fsgsd[2,3],
                    fsgN[3,3], fsgmean[3,3], fsgsd[3,3],
                    fsgN[1,3], fsgmean[1,3], fsgsd[1,3])
tableprint[5, ] = c("FSG High", fsgN[2,1], fsgmean[2,1], fsgsd[2,1],
                    fsgN[3,1], fsgmean[3,1], fsgsd[3,1],
                    fsgN[1,1], fsgmean[1,1], fsgsd[1,1])
tableprint[6, ] = c("LSA Low", lsaN[2,2], lsamean[2,2], lsasd[2,2],
                    lsaN[3,2], lsamean[3,2], lsasd[3,2],
                    lsaN[1,2], lsamean[1,2], lsasd[1,2])
tableprint[7, ] = c("LSA Average", lsaN[2,3], lsamean[2,3], lsasd[2,3],
                    lsaN[3,3], lsamean[3,3], lsasd[3,3],
                    lsaN[1,3], lsamean[1,3], lsasd[1,3])
tableprint[8, ] = c("LSA High", lsaN[2,1], lsamean[2,1], lsasd[2,1],
                    lsaN[3,1], lsamean[3,1], lsasd[3,1],
                    lsaN[1,1], lsamean[1,1], lsasd[1,1])

apa_table.latex(as.data.frame(tableprint),
                align = c("l", rep("c", 9)), 
                caption = "Summary Statistics for Stimuli",
                note = "COS: Cosine, FSG: Forward Strength, LSA: Latent Semantic Analysis.",
                escape = FALSE,
                col.names = c("Variable", " ", "COS Low", " ", " ", 
                              "COS Average", " ", " ", "COS High", " ")
                )
```

```{r descriptives, include=FALSE}
####setup####
dat1 = read.csv("stimuli jan 18.csv") ##original dataset
dat2 = read.csv("everything_data.csv") ##select the everything dataset, r was being a bitch and wouldn't open it normally...

####descriptives####
##network norms (part 2 data only)
round(apply(dat1[ , 3:5], 2, mean), digits = 2)
round(apply(dat1[ , 3:5], 2, sd), digits = 2)
round(apply(dat1[ , 3:5], 2, min), digits = 2)
round(apply(dat1[ , 3:5], 2, max), digits = 2)

##sw norms for cue items (combined data)
round(apply(dat2[ , c(7:13, 15:23, 41, 43)], 2, mean, na.rm = T), digits = 2)
round(apply(dat2[ , c(7:13, 15:23, 41, 43)], 2, sd, na.rm = T), digits = 2)
round(apply(dat2[ , c(7:13, 15:23, 41, 43)], 2, min, na.rm = T), digits = 2)
round(apply(dat2[ , c(7:13, 15:23, 41, 43)], 2, max, na.rm = T), digits = 2)

##sw norms for target items (combined data)
round(apply(dat2[ , c(24:29, 31:40, 42, 44)], 2, mean, na.rm = T), digits = 2)
round(apply(dat2[ , c(24:29, 31:40, 42, 44)], 2, sd, na.rm = T), digits = 2)
round(apply(dat2[ , c(24:29, 31:40, 42, 44)], 2, min, na.rm = T), digits = 2)
round(apply(dat2[ , c(24:29, 31:40, 42, 44)], 2, max, na.rm = T), digits = 2)
```

```{r stim-tables, eval=FALSE, include=FALSE, results='asis'}
####network norms table####
#this table shows the overall norms but we didn't include it because it's better to have them separated out in the Stimuli-Table section
tableprint1 = matrix(NA, nrow = 3, ncol = 6)
colnames(tableprint1) = c("Variable", "Citation", "Mean", 
                         "SD", "Min", "Max")

tableprint1[1, ] = c("FSG", "Nelson, McEvoy, and Schrieber, 2004", .13, .19, .01, .83)
tableprint1[2, ] = c("COS", "Maki, McKinley, and Thompson, 2004", .38, .20, .05, .88)
tableprint1[3, ] = c("LSA", "Landauer and Dumais, 1997", .42, .29, .00, .84)

tableprint1[ , 3:6] = printnum(as.numeric(tableprint1[ , 3:6]))

apa_table(tableprint1,
          align = c(rep("l", 2), rep("c", 4)), 
          caption = "Summary Statistics for Network Norms",
          note = "COS: Cosine, FSG: Forward Strength, LSA: Latent Semantic Analysis."
)
```

```{r cue-table, echo=FALSE, results='asis'}
####sw cue items####
tableprint2 = matrix(NA, nrow = 16, ncol = 6)
colnames(tableprint2) = c("Variable", "Citation", "Mean", 
                         "SD", "Min", "Max")

tableprint2[1, ] = c("QSS", "Nelson et al., 2004", 14.75, 4.49, 4.00, 27.00)
tableprint2[2, ] = c("Concreteness", "Nelson et al., 2004", 5.27, 1.09, 1.98, 7.00)
tableprint2[3, ] = c("LOG HAL Frequency", "Lund and Burgess, 1996", 9.33, 1.70, 5.73, 14.09)
tableprint2[4, ] = c("LOG SUBTLEX Frequency", "Brysbaert and New, 2009", 3.14, 0.77, 1.34, 5.35)
tableprint2[5, ] = c("Length", "Buchanan et al., 2013", 5.08, 1.43, 3.00, 10.00)
tableprint2[6, ] = c("Ortho N", "Buchanan et al., 2013", 6.44, 5.79, 0.00, 20.00)
tableprint2[7, ] = c("Phono N", "Buchanan et al., 2013", 16.55, 14.38, 0.00, 51.00)
tableprint2[8, ] = c("Phonemes", "Buchanan et al., 2013", 4.11, 1.32, 2.00, 9.00)
tableprint2[9, ] = c("Syllables", "Buchanan et al., 2013", 1.43, 0.67, 1.00, 4.00)
tableprint2[10, ] = c("Morphemes", "Buchanan et al., 2013", 1.06, 0.24, 1.00, 2.00)
tableprint2[11, ] = c("AOA", "Kuperman et al., 2012", 5.50, 1.75, 2.47, 11.05)
tableprint2[12, ] = c("Valence", "Warriner et al., 2013", 5.69, 1.17, 1.91, 7.89)
tableprint2[13, ] = c("Imageability", "Toglia and Battig, 1978", 5.41, 0.76, 3.02, 6.61)
tableprint2[14, ] = c("Familiarity", "Toglia and Battig, 1978", 6.18, 0.29, 5.30, 6.79)
tableprint2[15, ] = c("FSS", "Buchanan et al., 2018", 15.04, 10.47, 5.00, 48.00)
tableprint2[16, ] = c("COSC", "Buchanan et al., 2018", 81.94, 73.59, 1.00, 347.00)

tableprint2[ , 3:6] = printnum(as.numeric(tableprint2[ , 3:6]))

apa_table(tableprint2,
          align = c(rep("l", 2), rep("c", 4)), 
          caption = "Summary Statistics of Single Word Norms for Cue Items",
          note = "QSS: Cue Set Size, Ortho N: Orthographic Neighborhood Size, Phono N: Phonographic Neighborhood Size, AOA: Age of Acquisition, FSS: Feature Set Size, COSC: Cosine Connectedness"
)
```

```{r target-table, echo=FALSE, results='asis'}
####sw target items####
tableprint3 = matrix(NA, nrow = 16, ncol = 6)
colnames(tableprint3) = c("Variable", "Citation", "Mean", 
                          "SD", "Min", "Max")

tableprint3[1, ] = c("TSS", "Nelson et al., 2004", 14.79, 5.06, 4.00, 29.00)
tableprint3[2, ] = c("Concreteness", "Nelson et al., 2004", 5.35, 1.05, 1.28, 7.00)
tableprint3[3, ] = c("LOG HAL Frequency", "Lund and Burgess, 1996", 9.73, 1.53, 6.05, 13.08)
tableprint3[4, ] = c("LOG SUBTLEX Frequency", "Brysbaert and New, 2009", 3.34, 0.68, 1.59, 5.36)
tableprint3[5, ] = c("Length", "Buchanan et al., 2013", 4.81, 1.67, 2.00, 10.00)
tableprint3[6, ] = c("Ortho N", "Buchanan et al., 2013", 8.10, 7.47, 0.00, 29.00)
tableprint3[7, ] = c("Phono N", "Buchanan et al., 2013", 19.16, 15.93, 0.00, 59.00)
tableprint3[8, ] = c("Phonemes", "Buchanan et al., 2013", 3.86, 1.50, 1.00, 10.00)
tableprint3[9, ] = c("Syllables", "Buchanan et al., 2013", 1.35, 0.65, 1.00, 4.00)
tableprint3[10, ] = c("Morphemes", "Buchanan et al., 2013", 1.06, 0.23, 1.00, 2.00)
tableprint3[11, ] = c("AOA", "Kuperman et al., 2012", 4.92, 1.66, 2.47, 11.63)
tableprint3[12, ] = c("Valence", "Warriner et al., 2013", 5.81, 1.13, 1.95, 7.89)
tableprint3[13, ] = c("Imageability", "Toglia and Battig, 1978", 5.46, 0.75, 2.95, 6.45)
tableprint3[14, ] = c("Familiarity", "Toglia and Battig, 1978", 6.28, 0.29, 5.19, 6.85)
tableprint3[15, ] = c("FSS", "Buchanan et al., 2018", 16.58, 12.95, 5.00, 57.00)
tableprint3[16, ] = c("COSC", "Buchanan et al., 2018", 91.28, 89.90, 2.00, 462.00)

tableprint3[ , 3:6] = printnum(as.numeric(tableprint3[ , 3:6]))

apa_table(tableprint3,
          align = c(rep("l", 2), rep("c", 4)), 
          caption = "Summary Statistics of Single Word Norms for Target Items",
          note = "TSS: Target Set Size, Ortho N: Orthographic Neighborhood Size, Phono N: Phonographic Neighborhood Size, AOA: Age of Acquisition, FSS: Feature Set Size, COSC: Cosine Connectedness"
)
```

## Procedure

  The present study was divided into three phases. In the first section, participants were presented with word pairs and were asked to make judgments of how related they believed the words in each pair to be. This judgment phase consisted of three blocks of 21 word-pairs which corresponded to one of three types of word pair relationships: associative, semantic, or thematic. Each block was preceded by a set of instructions explaining one of the three types of relationships, and participants were provided with examples which illustrated the type of relationship to be judged. Judgment instructions for each block were contingent on the type of judgment being elicited. For example, instructions in the associative block asked participants to estimate how many college students out of 100 would respond to the cue word with the given target, while instructions for the semantic judgments asked participants to indicate the percent of features shared between two concepts. All judgment instructions were modeled after @Buchanan2010 and @Valentine2013.
  
  The association instruction set included the following instructional explanation focusing on the co-occurrence in language: "For example, consider the word (and concept of) DOG.  We often see the word DOG appear in the same context as the word CAT. "It's raining cats and dogs." "I have two dogs, but my neighbor has a cat."  And so on.  By experiencing the words CAT and DOG together many times, we develop an association (a mental connection) between them.  With lots of this kind of associative learning experience during our lives, we develop a very large and very complex associative memory."
  
  The semantic instructions focused on the definition and feature overlap of a set of concepts: "Consider the following words (and concepts) TORTOISE, TURTLE, SNAIL, and BANNER. We know that a TORTOISE is a reptile with an exoskeleton and a hard shell. If we compare the word TORTOISE with the word TURTLE, we find that they share a majority of the same features. Therefore, their definitions or characteristics overlap greatly."
  
  The thematic instructions contained a blend of the two instruction sets to focus on both semantic and associative relation: "Words that are thematically related are connected by a related concept and may often occur near each other in language.  For example, the word TREE is thematically related to LEAF, FRUIT, BRANCH, and FOREST because they all appear in text together due to related meaning. TREE and COMPUTER would not be thematically related because they would not be in the same writing together."

  After reviewing the intructions, participants rated the relatedness of the word pairs based on the set of instructions they received. Consistent with previous work investingatin JOLs and JAM tasks, item relatedness was rated using a scale of zero to 100, in which zero indicated a lack of relationship between items, and 100 indicated a perfect relationship. Participants typed their responses into the survey. After completing the first judgment block, participants then progressed through the remaining judgment blocks in the same manner. Each subsequent judgment block changed the type of judgment, and three versions of the study were created to counter-balance judgment order. Participants were randomly assigned to one of the three survey versions. Thus, each word-pair received judgments for each of the three types of relationships across participants.
  
  After progressing through the three judgment blocks, participants then completed a short distractor task to account for recency effects. This task required participants to place a randmized list of the 50 U.S. states in alphabetical order. This distractor was a timed task, designed to last two minutes. Participants automatically progressed to the final section of the study once time had elapsed. The final section of the study consisted of a cued-recall task in which participants were presented with each of the 63 cue words from the judgment phase. Participents were asked to complete each word-pair by responding with the correct target word. This section was an untimed task, and participants were informed that they would not be penalized for guessing. All stimuli pairs were were presented in a randomized order.

# Results

## Data Processing

```{r Data_Screeing, include=FALSE}
##Import data and setup
dat = read.csv("master.csv")

options(scipen = 999)
options(max.print = 9999)

####data screening#####
####part 2 data####
##accuracy
summary(dat)

table(dat$Judged.Value)

dat$Judged.Value[ dat$Judged.Value > 100 ] = NA
summary(dat$Judged.Value)

##missing data
table("judge" = is.na(dat$Judged.Value), "recall" = is.na(dat$Recall))
##12444 data points being used, 1479 excluded

nrow(na.omit(dat))

##outliers
mahal = mahalanobis(dat[ , c(4,5)], 
                    colMeans(dat[ , c(4,5)], na.rm = TRUE),
                    cov(dat[ , c(4,5)], use = "pairwise.complete.obs"))
cutoff = qchisq(1-.001, ncol(dat[ , c(4,5)]))
cutoff;ncol(dat[ , c(4,5)])
summary(mahal < cutoff) 
noout = subset(dat, mahal < cutoff)

##additivity
cor(noout[ , c(4:15, 17:31, 33:46)], use = "pairwise.complete.obs")

####pilot data####
pilot.dat = read.csv("Melted Data pilot.csv")

##data screening
##accuracy
summary(pilot.dat)

pilot.dat$Judged.Value[ pilot.dat$Judged.Value > 100 ] = NA
summary(pilot.dat$Judged.Value)

##missing pilot.data
table("judge" = is.na(pilot.dat$Judged.Value), "recall" = is.na(pilot.dat$Recall))

##outliers
mahal = mahalanobis(pilot.dat[ , c(4,5)], 
                    colMeans(pilot.dat[ , c(4,5)], na.rm = TRUE),
                    cov(pilot.dat[ , c(4,5)], use = "pairwise.complete.obs"))
cutoff = qchisq(1-.001, ncol(pilot.dat[ , c(4,5)]))
cutoff;ncol(pilot.dat[ , c(4,5)])
summary(mahal < cutoff) 
noout2 = subset(pilot.dat, mahal < cutoff)

```

```{r Descriptives, include=FALSE}
m.recall = tapply(noout$Recall, noout$Judgment, mean)
sd.recall = tapply(noout$Recall, noout$Judgment, sd)

m.judge = tapply(noout$Judged.Value, noout$Judgment, mean)
sd.judge = tapply(noout$Judged.Value, noout$Judgment, sd)

m.recall;sd.recall
m.judge;sd.judge

####scaling and centering####
####part 2 data####
##mean center variables
noout$ZCOS = scale(noout$COS, scale = F)
noout$ZLSA = scale(noout$LSA, scale = F)
noout$ZFSG = scale(noout$FSG, scale = F)

##create the right scaling 
noout$Judged.Value2 = noout$Judged.Value/100

##add in variables for moderations
noout$ZCOS_low = noout$ZCOS + sd(noout$ZCOS, na.rm = TRUE)
noout$ZCOS_high = noout$ZCOS - sd(noout$ZCOS, na.rm = TRUE)
noout$ZLSA_low = noout$ZLSA + sd(noout$ZLSA, na.rm = TRUE)
noout$ZLSA_high = noout$ZLSA - sd(noout$ZLSA, na.rm = TRUE)

####pilot data####
##mean centering network norms
noout2$ZCOS = scale(noout2$COS, scale = F)
noout2$ZLSA = scale(noout2$LSA, scale = F)
noout2$ZFSG = scale(noout2$FSG, scale = F)

##create the right scaling 
noout2$Judged.Value2 = noout2$Judged.Value/100

##add in variables for moderations
noout2$ZCOS_low = noout2$ZCOS + sd(noout2$ZCOS, na.rm = TRUE)
noout2$ZCOS_high = noout2$ZCOS - sd(noout2$ZCOS, na.rm = TRUE)
noout2$ZLSA_low = noout2$ZLSA + sd(noout2$ZLSA, na.rm = TRUE)
noout2$ZLSA_high = noout2$ZLSA - sd(noout2$ZLSA, na.rm = TRUE)

####combine everything####
colnames(noout2)[2] = "Judgment"
combined = rbind(noout, noout2)

summary(combined)

table("judge" = is.na(combined$Judged.Value), "recall" = is.na(combined$Recall))

##fix POS
combined$POS.2 = gsub("JJ", "ADJ", combined$POS.2)
combined$POS.2 = as.factor(combined$POS.2)
print(levels(combined$POS.2))

combined$POS.2 = factor(combined$POS.2,levels(combined$POS.2)[c(2, 1, 3, 4)])
print(levels(combined$POS.2))
                        
combined$POS.1 = factor(combined$POS.1,levels(combined$POS.1)[c(2, 1, 3, 4)])
print(levels(combined$POS.1))

##fix judgment type
summary(combined$Judgment)

combined$Judgment = gsub("semantic", "Semantic", combined$Judgment)
combined$Judgment = gsub("associative", "Associative", combined$Judgment)
combined$Judgment = gsub("thematic", "Thematic", combined$Judgment)

combined$Judgment = as.factor(combined$Judgment)

summary(combined$Judgment)

##notes
##use noout for network only models, combined for sw models
##for sw models, only using length, using log subtlex
```

Recall was coded as zero for incorrect responses, one for correct responses, and NA for participants who left either all or the majority of recall responses incomplete. All word responses to judgment items were deleted and set to missing data, as well as numerical rating responses on the cued-recall task. The final dataset was created by splitting the initial data file into six sections (corresponding to each of three experimental blocks and their respective recall sections) and individually melting each section using the reshape package in *R* [@Wickham2007]. Melted files were then written as *.csv* files and combined to create the final dataset.

In long format, the dataset for contained 13,923 rows of data (221 participants \* 63 judgments). Nine judgment data points were set to NA as they fell outside the range of the scale (> 100). Missing data points for judgments and recall were then excluded. 1,479 data points were excluded from the final analysis (831 from judgment only, 400 from recall only, and 248 across both), leading to a total of 12,444 observations from 211 participants in the final dataset. Recall and judgment scores were then screened for outliers using Mahalanobis distance at *p* < .001, and no outliers were detected. 12,444 data points remained in the final data set. Finally, multicollinearity was screened for by checking the correlations between network norms and single word norms. Because of high correlations between the various lexical measures representing word length (number of characters, syllables, morphemes, and phonemes, *r*s >.75), only number of individual characters was included in the analysis to represent word length.

Consistent with previous findings, the mean judgment of memory in the associative condition (*M* = `r printnum(m.judge["Associative"])`, *SD* = `r printnum(sd.judge["Associative"])`) was lower than in the semantic (*M* = `r printnum(m.judge["Semantic"])`, *SD* = `r printnum(sd.judge["Semantic"])`) and thematic (*M* = `r printnum(m.judge["Thematic"])`, *SD* = `r printnum(sd.judge["Thematic"])`) judgment conditions. Additionally, recall was at or slightly below 60% for all three conditions: associative *M* = `r printnum(m.recall["Associative"]*100)`, *SD* = `r printnum(sd.recall["Associative"]*100)`; semantic *M* = `r printnum(m.recall["Semantic"]*100)`, *SD* = `r printnum(sd.recall["Semantic"]*100)`; thematic *M* = `r printnum(m.recall["Thematic"]*100)`, *SD* = `r printnum(sd.recall["Thematic"]*100)`.

When examining the effects of single word norms, the dataset was combined with the dataset used collected in the pilot study, which had been updated to contain information corresponding to each of the single word norms being investigated. This additional step was taken because stimuli selected for use in this study also had to be included across several unconnected databases of single word norms, which severely limited which words could be selected. For example, of the 63 pairs originally selected, 56 word pairs contained weak associative relationships, six with moderate associative overlap, and only one pair with high associative overlap. To help control for this limitation, the single word norm analyses used a combined data set where single word norms were gathered for the stimuli used in the pilot study. The combined dataset contained a total of 18,713 data points collected across 317 participants (after excluding participants in data screening). This dataset was used for the analyses investigating the effects of single word norms, and data screening for the original data is described in @Maxwell2018.

## Replication of Interactions

### Judgments 

First, a set of analyses was conducted to test whether interactions between database norms would replicate with the new stimuli set. In the first analysis, multilevel modeling was used to test for these interactions when predicting participant judgements, while the second analysis used a multilevel logistic regression to test for interactions when predicting participant recall. Multilevel techniques were chosen due to their ability to retain all data points, rather than averaging across items and conditions. Additionally, such models control for correlated error due to participants, and are advantageous for multiway repeated measures designs [@Gelman2006]. All database norms were mean centered, and each analysis followed the design of @Maxwell2018. 

```{r judgment-models, include=FALSE}
####Judgment Models here####
overall.judge = lme(Judged.Value2 ~ Judgment + 
                      ZCOS * ZLSA * ZFSG, 
                    data = noout, 
                    method = "ML", 
                    na.action = "na.omit",
                    random = ~1|Partno)
summary(overall.judge) #partial replication, may be due to sampling error, etc

####simple slopes - judgments####
#break down cos (moved this section up)
#noout$ZCOS_low = noout$ZCOS + sd(noout$ZCOS, na.rm = TRUE)
#noout$ZCOS_high = noout$ZCOS - sd(noout$ZCOS, na.rm = TRUE)

##low cosine
lowcos = lme(Judged.Value2 ~ Judgment +
               ZCOS_low * ZLSA * ZFSG,
             data = noout,
             method = "ML",
             na.action = "na.omit",
             random = ~1|Partno)
summary(lowcos)

##high cosine
highcos = lme(Judged.Value2 ~ Judgment +
                ZCOS_high * ZLSA * ZFSG,
              data = noout,
              method = "ML",
              na.action = "na.omit",
              random = ~1|Partno)
summary(highcos)

##now splitting by LSA (moved this section up)
#noout$ZLSA_low = noout$ZLSA + sd(noout$ZLSA, na.rm = TRUE)
#noout$ZLSA_high = noout$ZLSA - sd(noout$ZLSA, na.rm = TRUE)

##low cosine, low lsa
lowcoslowlsa = lme(Judged.Value2 ~ Judgment +
                     ZCOS_low * ZLSA_low * ZFSG, 
                   data = noout, 
                   method = "ML", 
                   na.action = "na.omit",
                   random = ~1|Partno)
summary(lowcoslowlsa)

##low high
lowcoshighlsa = lme(Judged.Value2 ~ Judgment +
                      ZCOS_low  * ZLSA_high * ZFSG,
                    data = noout,
                    method = "ML",
                    na.action = "na.omit",
                    random = ~1|Partno)

summary(lowcoshighlsa)

#avg low
avgcoslowlsa = lme(Judged.Value2 ~ Judgment +
                     ZCOS * ZLSA_low * ZFSG,
                   data = noout,
                   method = "ML",
                   na.action = "na.omit",
                   random = ~1|Partno)

summary(avgcoslowlsa)

##avg high
avgcoshighlsa = lme(Judged.Value2 ~ Judgment +
                      ZCOS  * ZLSA_high * ZFSG, 
                    data = noout, 
                    method = "ML", 
                    na.action = "na.omit",
                    random = ~1|Partno)
summary(avgcoshighlsa)

##high low
highcoslowlsa = lme(Judged.Value2 ~ Judgment +
                      ZCOS_high * ZLSA_low * ZFSG, 
                    data = noout, 
                    method = "ML", 
                    na.action = "na.omit",
                    random = ~1|Partno)
summary(highcoslowlsa)

##high high
highcoshighlsa = lme(Judged.Value2 ~ Judgment +
                       ZCOS_high  * ZLSA_high * ZFSG, 
                     data = noout, 
                     method = "ML", 
                     na.action = "na.omit",
                     random = ~1|Partno)
summary(highcoshighlsa)
```

```{r judge-effects-table, results='asis', echo=FALSE}
####main effects table for judgments####
tableprint = matrix(NA, nrow = 22, ncol = 5)
colnames(tableprint) = c("Variable", "$beta$", "$SE$", "$t$", "$p$")

tableprint[1:10 , 2:5] = summary(overall.judge)$tTable[ , -3]
tableprint[1:10, 1] = c("Intercept", "Semantic Judgments", "Thematic Judgments", dimnames(summary(overall.judge)$tTable[ , -3])[[1]][4:10] )
tableprint[11:13 , 2:5] = summary(lowcos)$tTable[ c(5,6,9), -3]
tableprint[11:13, 1] = c("Low COS ZLSA", 
                         "Low COS ZFSG", 
                         "Low COS ZLSA:ZFSG")
tableprint[14:16 , 2:5] = summary(highcos)$tTable[ c(5,6,9), -3]
tableprint[14:16, 1] = c("High COS ZLSA", 
                         "High COS ZFSG", 
                         "High COS ZLSA:ZFSG")
tableprint[17, ] = c("Low COS Low LSA ZFSG",
                     summary(lowcoslowlsa)$tTable[6, -3])
tableprint[18, ] = c("Low COS High LSA ZFSG",
                     summary(lowcoshighlsa)$tTable[6, -3])
tableprint[19, ] = c("Avg COS Low LSA ZFSG",
                     summary(avgcoslowlsa)$tTable[6, -3])
tableprint[20, ] = c("Avg COS High LSA ZFSG",
                     summary(avgcoshighlsa)$tTable[6, -3])
tableprint[21, ] = c("High COS Low LSA ZFSG",
                     summary(highcoslowlsa)$tTable[6, -3])
tableprint[22, ] = c("High COS High LSA ZFSG",
                     summary(highcoshighlsa)$tTable[6, -3])
tableprint[ , 2] = apa(as.numeric(tableprint[ , 2]),3,T) 
tableprint[ , 3] = apa(as.numeric(tableprint[ , 3]),3,T)
tableprint[ , 4] = apa(as.numeric(tableprint[ , 4]),3,T)
tableprint[ , 5] =  apply(as.data.frame(as.numeric(tableprint[ , 5])), 1, p.value)

apa_table.latex(as.data.frame(tableprint),
          align = c("l", rep("c", 4)), 
          small = T,
          caption = "MLM Statistics for Replication of Judgment Interaction",
          note = "Database norms were mean centered. The table shows main effects and interactions for database norms at low, average, and high levels of COS and LSA when predicting participant judgments.",
          escape = FALSE)

```

First, analyses were conducted to test whether interactions between database norms would replicate with the new stimuli set. All database norms were mean centered to control for multicollinearity. The nlme package was used to analyze the MLM with maximimum likelihood estimation [@Pinheiro2017]. The analysis included the main effects and interaction of FSG, COS, and LSA predicting judgment scores with the participant as a random intercept factor. Type of judgment was used as a control variable to account for judgment instructions. The three-way interaction between these variables did not replicate; however, the two-way interactions of each of the variables did replicate. Table \@ref(tab:judgment-effects-table) displays statistics from the main effects and interactions. Consistent with previous findings, FSG was the strongest predictor of judgments (*b* = `r printnum(summary(overall.judge)$tTable[6,1],digits=2,gt1 =F)`, *p* < .001). Although the interaction was not significant, simple slopes were calculated to assess the underlying relationship between FSG and LSA at each level of COS to see if it displayed a pattern similar to that found in the previous study. Figure \@ref(fig:judgment-graphs) displays this relationship. FSG became weaker with each increase of LSA strength at each of the three levels of semantic overlap; thus, only the competitive relationship between the two database norms replicated for this analysis.

```{r judgment-graphs, echo=FALSE, fig.cap = "Simple slopes graph displaying the slope of FSG when predicting participant judgments at low, average, and high LSA split by low, average, and high COS. All variables were mean centered.", fig.height=6, fig.width=6}
####judgment graphs####
##low cos
plot1 = ggplot(noout, aes(x = ZCOS_low, y = Judged.Value2)) +
  labs(x = "ZFSG", y = "Judgment Value") +
  scale_size_continuous(guide = FALSE) +
  geom_abline(aes(intercept = 0.564, slope = 0.575, linetype = "-1SD ZLSA")) +
  geom_abline(aes(intercept = 0.612, slope = 0.472, linetype = "Average ZLSA")) +
  geom_abline(aes(intercept = 0.659, slope = 0.369, linetype = "+1SD ZLSA")) +
  scale_linetype_manual(values = c("dotted", "dashed", "solid"),
                        breaks = c("-1SD ZLSA", "Average ZLSA", "+1SD ZLSA"),
                        name = "Simple Slope") +
  coord_cartesian(xlim = c(-.20, .60), ylim = c(.1, 1)) +
  geom_vline(xintercept = -.30) +
  geom_hline(yintercept = 0) +
  cleanup + 
  labs(title="Low ZCOS") 

##avg cos
plot2 = ggplot(noout, aes(x = ZCOS_low, y = Judged.Value2)) +
  labs(x = "ZFSG", y = "Judgment Value") +
  scale_size_continuous(guide = FALSE) +
  geom_abline(aes(intercept = 0.588, slope = 0.514, linetype = "-1SD ZLSA")) +
  geom_abline(aes(intercept = 0.615, slope = 0.422, linetype = "Average ZLSA")) +
  geom_abline(aes(intercept = 0.641, slope = 0.330, linetype = "+1SD ZLSA")) +
  scale_linetype_manual(values = c("dotted", "dashed", "solid"),
                        breaks = c("-1SD ZLSA", "Average ZLSA", "+1SD ZLSA"),
                        name = "Simple Slope") +
  coord_cartesian(xlim = c(-.20, .60), ylim = c(.1, 1)) +
  geom_vline(xintercept = -.30) +
  geom_hline(yintercept = 0) +
  cleanup + 
  labs(title="Average ZCOS")

##high cos
plot3 = ggplot(noout, aes(x = ZCOS_low, y = Judged.Value2)) +
  labs(x = "ZFSG", y = "Judgment Value") +
  scale_size_continuous(guide = FALSE) +
  geom_abline(aes(intercept = 0.612, slope = 0.453, linetype = "-1SD ZLSA")) +
  geom_abline(aes(intercept = 0.618, slope = 0.372, linetype = "Average ZLSA")) +
  geom_abline(aes(intercept = 0.624, slope = 0.292, linetype = "+1SD ZLSA")) +
  scale_linetype_manual(values = c("dotted", "dashed", "solid"),
                        breaks = c("-1SD ZLSA", "Average ZLSA", "+1SD ZLSA"),
                        name = "Simple Slope") +
  coord_cartesian(xlim = c(-.20, .60), ylim = c(.1, 1)) +
  geom_vline(xintercept = -.30) +
  geom_hline(yintercept = 0) +
  cleanup + 
  labs(title="High ZCOS") 

legend = get_legend(plot1)
judge.graph = plot_grid( plot1 + theme(legend.position="none"),
                           plot2 + theme(legend.position="none"),
                           plot3 + theme(legend.position="none"),
                           legend,
                           hjust = -1,
                           nrow = 2
)
judge.graph

##judgments using the new dataset are competitive at all levels of COS
##partial replication -- competitiveness replicates, complimentary aspect at high does not

```

### Recall

```{r recall-models, include=FALSE}
####recall models here####
overall.recall = glmer(Recall ~ (1|Partno) + Judgment + 
                         Judged.Value2 + ZCOS * ZLSA * ZFSG,
                       data = noout,
                       family = binomial,
                       control = glmerControl(optimizer = "bobyqa"),
                       nAGQ = 1)
summary(overall.recall) ##three way interaction still exists!

####moderation -- Recall####
#low cosine
lowcos2 = glmer(Recall ~ (1|Partno) +
                  Judgment + Judged.Value2 + ZCOS_low * ZLSA * ZFSG,
                data = noout,
                family = binomial,
                control = glmerControl(optimizer = "bobyqa"),
                nAGQ = 1)

##high cosine
hicos2 = glmer(Recall ~ (1|Partno) + 
                 Judgment + Judged.Value2 + ZCOS_high * ZLSA * ZFSG,
               data = noout,
               family = binomial,
               control = glmerControl(optimizer = "bobyqa"),
               nAGQ = 1)

summary(lowcos2)
summary(hicos2)

##low cosine low lsa
lowcoslowlsa2 = glmer(Recall ~ (1|Partno) + 
                        Judgment + Judged.Value2 + ZCOS_low * ZLSA_low * ZFSG,
                      data = noout,
                      family = binomial,
                      control = glmerControl(optimizer = "bobyqa"),
                      nAGQ = 1)
summary(lowcoslowlsa2)

##low cosine high lsa
lowcoshighlsa2 = glmer(Recall ~ (1|Partno) + 
                         Judgment + Judged.Value2 + ZCOS_low * ZLSA_high * ZFSG,
                       data = noout,
                       family = binomial,
                       control = glmerControl(optimizer = "bobyqa"),
                       nAGQ = 1)
summary(lowcoshighlsa2)

##high low
highcoslowlsa2 = glmer(Recall ~ (1|Partno) + 
                         Judgment + Judged.Value2 + ZCOS_high * ZLSA_low * ZFSG,
                       data = noout,
                       family = binomial,
                       control = glmerControl(optimizer = "bobyqa"),
                       nAGQ = 1)
summary(highcoslowlsa2)

##high high
highcoshighlsa2 = glmer(Recall ~ (1|Partno) + 
                          Judgment + Judged.Value2 + ZCOS_high * ZLSA_high * ZFSG,
                        data = noout,
                        family = binomial,
                        control = glmerControl(optimizer = "bobyqa"),
                        nAGQ = 1)
summary(highcoshighlsa2)

avgcoslowlsa2 = glmer(Recall ~ (1|Partno) + 
                        Judgment + Judged.Value2 + ZCOS * ZLSA_low * ZFSG,
                      data = noout,
                      family = binomial,
                      control = glmerControl(optimizer = "bobyqa"),
                      nAGQ = 1)
summary(avgcoslowlsa2)

avgcoshighlsa2 = glmer(Recall ~ (1|Partno) + 
                         Judgment + Judged.Value2 + ZCOS * ZLSA_high * ZFSG,
                       data = noout,
                       family = binomial,
                       control = glmerControl(optimizer = "bobyqa"),
                       nAGQ = 1) 
summary(avgcoshighlsa2)
```

```{r recall-effects-table, results='asis', echo=FALSE}
####main effects table for recall####
tableprint = matrix(NA, nrow = 21, ncol = 5)
colnames(tableprint) = c("Variable", "$beta$", "$SE$", "$z$", "$p$")

tableprint[1:11 , 2:5] = coef(summary(overall.recall))
tableprint[1:11, 1] = c("Intercept", "Semantic Judgments", "Thematic Judgments", "Judged Values", dimnames(coef(summary(overall.recall)))[[1]][5:11])
tableprint[12:14 , 2:5] = coef(summary(lowcos2))[c(6,7,10), ]
tableprint[12:14, 1] = c("Low COS ZLSA", 
                         "Low COS ZFSG", 
                         "Low COS ZLSA:ZFSG")
tableprint[15:17, 2:5] = coef(summary(hicos2))[c(6,7,10), ]
tableprint[15:17, 1] = c("High COS ZLSA", 
                         "High COS ZFSG", 
                         "High COS ZLSA:ZFSG")
tableprint[18, ] = c("Low COS Low LSA ZFSG", coef(summary(lowcoslowlsa2))[7,])
tableprint[19, ] = c("Low COS High LSA ZFSG", coef(summary(lowcoshighlsa2))[7,])
tableprint[20, ] = c("High COS Low LSA ZFSG", coef(summary(highcoslowlsa2))[7,])
tableprint[21, ] = c("High COS High LSA ZFSG", coef(summary(highcoshighlsa2))[7,])

tableprint[ , 2] = apa(as.numeric(tableprint[ , 2]),3,T) 
tableprint[ , 3] = apa(as.numeric(tableprint[ , 3]),3,T)
tableprint[ , 4] = apa(as.numeric(tableprint[ , 4]),3,T)
tableprint[ , 5] = apply(as.data.frame(as.numeric(tableprint[ , 5])), 1, p.value)


apa_table.latex(as.data.frame(tableprint),
          align = c("l", rep("c", 4)), 
          small = T,
          caption = "MLM Statistics for Replication of Recall Interaction",
          note = "Database norms were mean centered. The table shows main effects and interactions for database norms at low, average, and high levels of COS and LSA when predicting recall.",
          escape = FALSE)
```

The lme4 package was then used to create a multilevel logistic regression [CITE HERE], which tested whether the interaction found between the database norms when predicting recall would replicate with the new stimuli set. Participants were used as a random intercept factor with the same main effects and interactions of FSG, COS, and LSA. Again, the type of judgment was used as a control variable. Overall, a signicant three-way interaction was detected between FSG, COS, and LSA (*b* = `r printnum(coef(summary(overall.recall))[11,1])`, *p* < .001). This finding was a partial replication, as this interaction was in the opposite direction as the one found previously. Table \@ref(tab:recall-effects-table) reports main effects, two-way, and three-way interaction values. Simple slopes were then calculated for low, average, and high levels of LSA at the low and high levels of COS, so as to assess how FSG affected recall at varying levels of both COS and LSA. 

In line with findings from the previous experiment, these analyses yielded significant two-way interactions between LSA and FSG at low COS (*b* = `r printnum(coef(summary(lowcos2))[10,1])`, *p* = `r printp(coef(summary(lowcos2))[10,4])`) and high COS (b = -7.514, p < .001), with no significant two-way interaction being found at average COS (*b* = `r printnum(coef(summary(overall.recall))[10,1])`, *p* = `r printp(coef(summary(overall.recall))[10,4])`). Staying consistent with the process used previously, a second set of simple slopes were then calculated for low, average, and high levels of LSA at the low and high levels of COS, so as to assess how FSG affected recall at varying levels of both COS and LSA. In contrast to previous findings, when both COS and LSA were low, FSG did not predict recall (*b* = `r printnum(coef(summary(lowcoslowlsa2))[7,1])`, *p* = `r printp(coef(summary(lowcoslowlsa2))[7,4])`). At low COS and average LSA, FSG increased in strength and became a significant predictor (*b* = `r printnum(coef(summary(lowcos2))[7,1])`, *p* < `r printp(coef(summary(lowcos2))[7,4])`). Finally, at low COS and high LSA, FSG increased further as a predictor (*b* = `r printnum(coef(summary(lowcoshighlsa2))[7,1])`, *p* `r printp(coef(summary(lowcoshighlsa2))[7,4])`). 

The observed interaction followed a trend opposite of that the pilot study. Instead of the competitive relationship observed previously for low COS, LSA and FSG were complimentary and increased together. As COS increased FSG and LSA became competitive, which was the opposite of previous results.

As such, at high COS and low LSA, FSG was a significant predictor
(*b* = `r printnum(coef(summary(highcoslowlsa2))[7,1])`, *p* `r printp(coef(summary(highcoslowlsa2))[7,4])`). FSG weakened when LSA increased to average levels, (*b* = `r printnum(coef(summary(hicos2))[7,1])`, *p* `r printp(coef(summary(hicos2))[7,4])`), and continued to weaken further when both COS and LSA were high, with FSG decreasing further as a predictor of recall (*b* = `r printnum(coef(summary(highcoshighlsa2))[7,1])`, *p* = `r printp(coef(summary(highcoshighlsa2))[7,4])`). Figure @\ref(fig:recall-figure) displays simple slopes graphs for the three-way interaction when predicting recall. The bottom left figure indicates the counterbalancing effect of high COS levels of LSA and FSG, while the top left figure displays the complementary effects where LSA and FSG increased together as predictors of recall at low COS levels. 

```{r recall-figure, echo=FALSE, fig.cap = "Simple slopes graph displaying the slope of FSG when predicting participant recall at low, average, and high LSA split by low, average, and high COS. All variables were mean centered.", fig.height=6, fig.width=6}
####Recall graphs####
##low cos
plot4 = ggplot(noout, aes(x = ZCOS_low, y = Recall)) +
  labs(x = "ZFSG", y = "Recall") +
  scale_size_continuous(guide = FALSE) +
  geom_abline(aes(intercept = 0.162, slope = 0.087, linetype = "-1SD ZLSA")) +
  geom_abline(aes(intercept = 0.118, slope = 1.213, linetype = "Average ZLSA")) +
  geom_abline(aes(intercept = 0.074, slope = 2.339, linetype = "+1SD ZLSA")) +
  scale_linetype_manual(values = c("dotted", "dashed", "solid"),
                        breaks = c("-1SD ZLSA", "Average ZLSA", "+1SD ZLSA"),
                        name = "Simple Slope") +
  coord_cartesian(xlim = c(-.20, .60), ylim = c(.1, 1)) +
  geom_vline(xintercept = -.30) +
  geom_hline(yintercept = 0) +
  cleanup + 
  labs(title="Low ZCOS") 

##avg cos
plot5 = ggplot(noout, aes(x = ZCOS_low, y = Recall)) +
  labs(x = "ZFSG", y = "Recall") +
  scale_size_continuous(guide = FALSE) +
  geom_abline(aes(intercept = 0.167, slope = 1.993, linetype = "-1SD ZLSA")) +
  geom_abline(aes(intercept = 0.303, slope = 1.800, linetype = "Average ZLSA")) +
  geom_abline(aes(intercept = 0.440, slope = 1.606, linetype = "+1SD ZLSA")) +
  scale_linetype_manual(values = c("dotted", "dashed", "solid"),
                        breaks = c("-1SD ZLSA", "Average ZLSA", "+1SD ZLSA"),
                        name = "Simple Slope") +
  coord_cartesian(xlim = c(-.20, .60), ylim = c(.1, 1)) +
  geom_vline(xintercept = -.30) +
  geom_hline(yintercept = 0) +
  cleanup + 
  labs(title="Average ZCOS")

##high cos
plot6 = ggplot(noout, aes(x = ZCOS_low, y = Recall)) +
  labs(x = "ZFSG", y = "Recall") +
  scale_size_continuous(guide = FALSE) +
  geom_abline(aes(intercept = .169, slope = 3.900, linetype = "-1SD ZLSA")) +
  geom_abline(aes(intercept = .487, slope = 2.386, linetype = "Average ZLSA")) +
  geom_abline(aes(intercept = .806, slope = 0.872, linetype = "+1SD ZLSA")) +
  scale_linetype_manual(values = c("dotted", "dashed", "solid"),
                        breaks = c("-1SD ZLSA", "Average ZLSA", "+1SD ZLSA"),
                        name = "Simple Slope") +
  coord_cartesian(xlim = c(-.20, .60), ylim = c(.1, 1)) +
  geom_vline(xintercept = -.30) +
  geom_hline(yintercept = 0) +
  cleanup + 
  labs(title="High ZCOS") 

legend = get_legend(plot4)
recall.graph =  plot_grid( plot4 + theme(legend.position="none"),
                           plot5 + theme(legend.position="none"),
                           plot6 + theme(legend.position="none"),
                           legend,
                           hjust = -1,
                           nrow = 2
)
recall.graph

##graph notes:
##FSG slopes get stronger as LSA increases at low COS
##FSG gets weaker as LSA increases at higher cosines.
##this is the opposite of the pilot
```

## Extension to Single Word Norms

The final group of analyses examined the effects of single word norms on recall and judgments and whether interaction findings would replicate after controlling for single word norms. These analyses were conducted using an expanded dataset which combined data collected across both experiments.

Single word norms were placed into one of three categories. Frequency (measured with SUBTLEX) and word length were used as measures of lexical information. Part of speech was excluded because this variable created a singular matrix. Age of acquisition, valence, familiarity, concreteness, and imageability were classified as rated properties. Orthographic and phonographic neighborhoods, cue and target set sizes, feature set size, and cosine connectedness were grouped together as neighborhood connections.

```{r sw-judgments, include=FALSE}
####Hierarchical Judgment Models####
judgeoverall.1 = lme(Judged.Value2 ~ Judgment + 
                      LogSub.1 + LogSub.2 + Length.1 + Length.2, #POS.1 + POS.2,
                     data = combined,
                     method = "ML", 
                     na.action = "na.omit",
                     random = ~1|Partno)
summary(judgeoverall.1, correlation = T)

##rated properties
judgeoverall.2 = lme(Judged.Value2 ~ Judgment + 
                       LogSub.1 + LogSub.2 + Length.1 + Length.2 + # POS.1 + POS.2,
                       AOA.1 +  AOA.2 +  Familiarity.1 +
                       Familiarity.2 +  Valence.1 +  Valence.2 + 
                       Imageability.1 +  Imageability.2 +  QCON.1 +  QCON.2,
                     data = combined,
                     method = "ML", 
                     na.action = "na.omit",
                     random = ~1|Partno)
summary(judgeoverall.2, correlation = T)

##neighborhood connections
judgeoverall.3 = lme(Judged.Value2 ~ Judgment + 
                       LogSub.1 + LogSub.2 + Length.1 + Length.2 + # POS.1 + POS.2,
                       AOA.1 +  AOA.2 +  Familiarity.1 +
                       Familiarity.2 +  Valence.1 +  Valence.2 + 
                       Imageability.1 +  Imageability.2 +  QCON.1 +  QCON.2 +
                       QSS.1 + TSS.2 + FSS.1 + FSS.2 + 
                       COSC.1 + COSC.2 + Ortho.1 + Ortho.2 + Phono.1 + Phono.2,
                     data = combined,
                     method = "ML", 
                     na.action = "na.omit",
                     random = ~1|Partno)
summary(judgeoverall.3, correlation = T)

##network norms
judgeoverall.4 = lme(Judged.Value2 ~ Judgment + 
                       LogSub.1 + LogSub.2 + Length.1 + Length.2 + # POS.1 + POS.2,
                       AOA.1 +  AOA.2 +  Familiarity.1 +
                       Familiarity.2 +  Valence.1 +  Valence.2 + 
                       Imageability.1 +  Imageability.2 +  QCON.1 +  QCON.2 +
                       QSS.1 + TSS.2 + FSS.1 + FSS.2 + 
                       COSC.1 + COSC.2 + Ortho.1 + Ortho.2 + Phono.1 + Phono.2 +
                       ZFSG * ZLSA * ZCOS,
                     data = combined,
                     method = "ML", 
                     na.action = "na.omit",
                     random = ~1|Partno)
summary(judgeoverall.4, correlation = T)

##notes:
##three way is not sig, two of the two ways are, FSG is still strongest predictor.
##these results replicate judgment results from just network norms

####R-Squared####
##playing w/ mumin package
library(MuMIn)

model.sel(judgeoverall.1, judgeoverall.2, judgeoverall.3, judgeoverall.4)
##got a warning about models being fitted to different datasets
##think this is due to having missing data in pilot sw norms

##marginal (r2m) = variance expalined by fixed factors
##conditional (r2c) = variance expalined by the entire model

r.squaredGLMM(judgeoverall.1)
r.squaredGLMM(judgeoverall.2)
r.squaredGLMM(judgeoverall.3)
r.squaredGLMM(judgeoverall.4) ##r-squared increaeses with each model

####sw judgment moderation####
##low cosine
lowcos3 = lme(Judged.Value2 ~ Judgment + 
                       LogSub.1 + LogSub.2 + Length.1 + Length.2 + # POS.1 + POS.2,
                       AOA.1 +  AOA.2 +  Familiarity.1 +
                       Familiarity.2 +  Valence.1 +  Valence.2 + 
                       Imageability.1 +  Imageability.2 +  QCON.1 +  QCON.2 +
                       QSS.1 + TSS.2 + FSS.1 + FSS.2 + 
                       COSC.1 + COSC.2 + Ortho.1 + Ortho.2 + Phono.1 + Phono.2 +
                ZFSG * ZLSA * ZCOS_low,
              data = combined,
              method = "ML", 
              na.action = "na.omit",
              random = ~1|Partno)
summary(lowcos3)

##high cosine
highcos3 = lme(Judged.Value2 ~ Judgment + 
                       LogSub.1 + LogSub.2 + Length.1 + Length.2 + # POS.1 + POS.2,
                       AOA.1 +  AOA.2 +  Familiarity.1 +
                       Familiarity.2 +  Valence.1 +  Valence.2 + 
                       Imageability.1 +  Imageability.2 +  QCON.1 +  QCON.2 +
                       QSS.1 + TSS.2 + FSS.1 + FSS.2 + 
                       COSC.1 + COSC.2 + Ortho.1 + Ortho.2 + Phono.1 + Phono.2 +
                 ZFSG * ZLSA * ZCOS_high,
               data = combined,
               method = "ML", 
               na.action = "na.omit",
               random = ~1|Partno)
summary(highcos3)

##low cosine, low lsa
lowcoslowlsa3 = lme(Judged.Value2 ~ Judgment + 
                       LogSub.1 + LogSub.2 + Length.1 + Length.2 + # POS.1 + POS.2,
                       AOA.1 +  AOA.2 +  Familiarity.1 +
                       Familiarity.2 +  Valence.1 +  Valence.2 + 
                       Imageability.1 +  Imageability.2 +  QCON.1 +  QCON.2 +
                       QSS.1 + TSS.2 + FSS.1 + FSS.2 + 
                       COSC.1 + COSC.2 + Ortho.1 + Ortho.2 + Phono.1 + Phono.2 +
                      ZFSG * ZLSA_low * ZCOS_low,
                    data = combined,
                    method = "ML", 
                    na.action = "na.omit",
                    random = ~1|Partno)
summary(lowcoslowlsa3)

##low high
lowcoshighlsa3 = lme(Judged.Value2 ~ Judgment + 
                       LogSub.1 + LogSub.2 + Length.1 + Length.2 + # POS.1 + POS.2,
                       AOA.1 +  AOA.2 +  Familiarity.1 +
                       Familiarity.2 +  Valence.1 +  Valence.2 + 
                       Imageability.1 +  Imageability.2 +  QCON.1 +  QCON.2 +
                       QSS.1 + TSS.2 + FSS.1 + FSS.2 + 
                       COSC.1 + COSC.2 + Ortho.1 + Ortho.2 + Phono.1 + Phono.2 +
                       ZFSG * ZLSA_high * ZCOS_low,
                     data = combined,
                     method = "ML", 
                     na.action = "na.omit",
                     random = ~1|Partno)
summary(lowcoshighlsa3)

#avg low
avgcoslowlsa3 = lme(Judged.Value2 ~ Judgment + 
                       LogSub.1 + LogSub.2 + Length.1 + Length.2 + # POS.1 + POS.2,
                       AOA.1 +  AOA.2 +  Familiarity.1 +
                       Familiarity.2 +  Valence.1 +  Valence.2 + 
                       Imageability.1 +  Imageability.2 +  QCON.1 +  QCON.2 +
                       QSS.1 + TSS.2 + FSS.1 + FSS.2 + 
                       COSC.1 + COSC.2 + Ortho.1 + Ortho.2 + Phono.1 + Phono.2 +
                      ZFSG * ZLSA_low * ZCOS,
                    data = combined,
                    method = "ML", 
                    na.action = "na.omit",
                    random = ~1|Partno)
summary(avgcoslowlsa3)

##avg high
avgcoshighlsa3 = lme(Judged.Value2 ~ Judgment + 
                       LogSub.1 + LogSub.2 + Length.1 + Length.2 + # POS.1 + POS.2,
                       AOA.1 +  AOA.2 +  Familiarity.1 +
                       Familiarity.2 +  Valence.1 +  Valence.2 + 
                       Imageability.1 +  Imageability.2 +  QCON.1 +  QCON.2 +
                       QSS.1 + TSS.2 + FSS.1 + FSS.2 + 
                       COSC.1 + COSC.2 + Ortho.1 + Ortho.2 + Phono.1 + Phono.2 +
                       ZFSG * ZLSA_high* ZCOS,
                     data = combined,
                     method = "ML", 
                     na.action = "na.omit",
                     random = ~1|Partno)
summary(avgcoshighlsa3)

##high low
highcoslowlsa3 = lme(Judged.Value2 ~ Judgment + 
                       LogSub.1 + LogSub.2 + Length.1 + Length.2 + # POS.1 + POS.2,
                       AOA.1 +  AOA.2 +  Familiarity.1 +
                       Familiarity.2 +  Valence.1 +  Valence.2 + 
                       Imageability.1 +  Imageability.2 +  QCON.1 +  QCON.2 +
                       QSS.1 + TSS.2 + FSS.1 + FSS.2 + 
                       COSC.1 + COSC.2 + Ortho.1 + Ortho.2 + Phono.1 + Phono.2 +
                       ZFSG * ZLSA_low * ZCOS_high,
                     data = combined,
                     method = "ML", 
                     na.action = "na.omit",
                     random = ~1|Partno)
summary(highcoslowlsa3)

##high high
highcoshighlsa3 = lme(Judged.Value2 ~ Judgment + 
                       LogSub.1 + LogSub.2 + Length.1 + Length.2 + # POS.1 + POS.2,
                       AOA.1 +  AOA.2 +  Familiarity.1 +
                       Familiarity.2 +  Valence.1 +  Valence.2 + 
                       Imageability.1 +  Imageability.2 +  QCON.1 +  QCON.2 +
                       QSS.1 + TSS.2 + FSS.1 + FSS.2 + 
                       COSC.1 + COSC.2 + Ortho.1 + Ortho.2 + Phono.1 + Phono.2 +
                        ZFSG * ZLSA_high * ZCOS_high,
                      data = combined,
                      method = "ML", 
                      na.action = "na.omit",
                      random = ~1|Partno)
summary(highcoshighlsa3)
```

```{r sw.judge.effects.table, include=FALSE}
####main effects table for sw judgments####
#since there is no way any of this noise will fit into a publishable table, we shall 
#print it out in excel

swjudgment = rbind(
  cbind(rep("Step 1", nrow(summary(judgeoverall.1)$tTable)), summary(judgeoverall.1)$tTable),
cbind(rep("Step 2", nrow(summary(judgeoverall.2)$tTable)), summary(judgeoverall.2)$tTable),
cbind(rep("Step 3", nrow(summary(judgeoverall.3)$tTable)), summary(judgeoverall.3)$tTable),
cbind(rep("Step 4", nrow(summary(judgeoverall.4)$tTable)), summary(judgeoverall.4)$tTable),
cbind(rep("Low COS Low LSA", nrow(summary(lowcoslowlsa3)$tTable[28:34, ])), summary(lowcoslowlsa3)$tTable[28:34, ]),
cbind(rep("Low COS Average LSA", nrow(summary(lowcos3)$tTable[28:34, ])), summary(lowcos3)$tTable[28:34, ]),
cbind(rep("Low COS High LSA", nrow(summary(lowcoshighlsa3)$tTable[28:34, ])), summary(lowcoshighlsa3)$tTable[28:34, ]),
cbind(rep("High COS Low LSA", nrow(summary(highcoslowlsa3)$tTable[28:34, ])), summary(highcoslowlsa3)$tTable[28:34, ]),
cbind(rep("High COS Average LSA", nrow(summary(highcos3)$tTable[28:34, ])), summary(highcos3)$tTable[28:34, ]),
cbind(rep("High COS High LSA", nrow(summary(highcoshighlsa3)$tTable[28:34, ])), summary(highcoshighlsa3)$tTable[28:34, ]))

write.csv(swjudgment, "single word judgments.csv", row.names = T)

```

YOU SHOULD EDIT HERE

Next, multilevel modeling was used to investigate whether interaction findings from Experiment One would replicate after controlling for each of the single word norms selected via stepwise analyses. This analysis was conducted hierarchically, with single word norms entered in to the model through a series of steps. Each step corresponded to one of the categories of single word norms, with each model using judgment scores as the dependent variable of interest and controlling for the type of judgment being made. Marginal and Conditional R2 values (R2m and R2c respectively) were calculated at each step of the judgement model using the MuMIn package (Barton, 2018). Marginal R2 describes the proportion of variance that is explained solely by the fixed factors in the model, while the conditional R2 value is used to describe the proportion of the variance that can be explained by both fixed and random factors (Lefcheck, 2013).

Model one examined the lexical properties of words (R2m = .027, R2c = .194). The second model added rated word properties words (R2m = .054, R2c = .220), and the third model added in neighborhood connections (R2m = .068, R2c = .232). Network norms and the three-way interaction between them were entered into the analysis in the fourth and final model (R2m = .118, R2c = .283). 

Table 12 displays main effects and interaction findings for all variables in the step they were entered to control for table size. The main investigation focused on the fourth and final step of the model with the network interaction. The main effects of each individual single word norm are not discussed, however, of notable interest is the way in which several single word predictors tended to balance out across cue and target items. This finding occurred when either the cue or target version of a particular single word norm predictor showed a positive relationship, while the other displayed a negative relationship. Several cue-target predictor pairs displaying this trend were found at each step of the model. Pairs following this trend included frequency (cue b = .014, p < .001; target b = -.032, p < .001), age of acquisition (cue b = .015, p < .001; target b = -.014, p < .001), and feature set size (cue b = .001, p < .001, target b = -.001, p < .001). Therefore, even though it appeared that many features related to single words were significant predictors of judgments, the related cue and target information often canceled each other out in strength.  Consistent with previous judgment models, FSG was found in the final step to be the strongest overall predictor of judgments (b = .391, p < .001). The three-way interaction between network norms was not significant (b = .558, p = .099). To explore potential differences in effects, simple slopes were calculated using the same process as before. Figure 8 displays these findings.  FSG and LSA strength were competitive at all levels of COS, with increases in thematic strength decreasing the overall predictiveness of association strength. These results matched the replication portion of this experiment, indicating that FSG and LSA competition findings still hold, even after controlling for other concept information that is activated when reading in the lexical network. 

```{r sw judgment graphs, include=FALSE}
####judgment graphs####
##low cos
plot7 = ggplot(noout, aes(x = ZCOS_low, y = Judged.Value2)) +
  labs(x = "ZFSG", y = "Judgment Value") +
  scale_size_continuous(guide = FALSE) +
  geom_abline(aes(intercept = 0.818, slope = 0.524, linetype = "-1SD ZLSA")) +
  geom_abline(aes(intercept = 0.866, slope = 0.419, linetype = "Average ZLSA")) +
  geom_abline(aes(intercept = 0.914, slope = 0.309, linetype = "+1SD ZLSA")) +
  scale_linetype_manual(values = c("dotted", "dashed", "solid"),
                        breaks = c("-1SD ZLSA", "Average ZLSA", "+1SD ZLSA"),
                        name = "Simple Slope") +
  coord_cartesian(xlim = c(-.20, .60), ylim = c(.1, 1)) +
  geom_vline(xintercept = -.30) +
  geom_hline(yintercept = 0) +
  cleanup + 
  labs(title="Low ZCOS") 

##avg cos
plot8 = ggplot(noout, aes(x = ZCOS_low, y = Judged.Value2)) +
  labs(x = "ZFSG", y = "Judgment Value") +
  scale_size_continuous(guide = FALSE) +
  geom_abline(aes(intercept = 0.847, slope = 0.469, linetype = "-1SD ZLSA")) +
  geom_abline(aes(intercept = 0.874, slope = 0.391, linetype = "Average ZLSA")) +
  geom_abline(aes(intercept = 0.901, slope = 0.311, linetype = "+1SD ZLSA")) +
  scale_linetype_manual(values = c("dotted", "dashed", "solid"),
                        breaks = c("-1SD ZLSA", "Average ZLSA", "+1SD ZLSA"),
                        name = "Simple Slope") +
  coord_cartesian(xlim = c(-.20, .60), ylim = c(.1, 1)) +
  geom_vline(xintercept = -.30) +
  geom_hline(yintercept = 0) +
  cleanup + 
  labs(title="Average ZCOS")

##high cos
plot9 = ggplot(noout, aes(x = ZCOS_low, y = Judged.Value2)) +
  labs(x = "ZFSG", y = "Judgment Value") +
  scale_size_continuous(guide = FALSE) +
  geom_abline(aes(intercept = 0.877, slope = 0.409, linetype = "-1SD ZLSA")) +
  geom_abline(aes(intercept = 0.883, slope = 0.361, linetype = "Average ZLSA")) +
  geom_abline(aes(intercept = 0.888, slope = 0.316, linetype = "+1SD ZLSA")) +
  scale_linetype_manual(values = c("dotted", "dashed", "solid"),
                        breaks = c("-1SD ZLSA", "Average ZLSA", "+1SD ZLSA"),
                        name = "Simple Slope") +
  coord_cartesian(xlim = c(-.20, .60), ylim = c(.1, 1)) +
  geom_vline(xintercept = -.30) +
  geom_hline(yintercept = 0) +
  cleanup + 
  labs(title="High ZCOS") 

legend = get_legend(plot7)
judge.graph.sw = plot_grid( plot7 + theme(legend.position="none"),
                         plot8 + theme(legend.position="none"),
                         plot9 + theme(legend.position="none"),
                         legend,
                         hjust = -1,
                         nrow = 2
)
judge.graph.sw

```

```{r sw-recall, include = FALSE}
####hierarchical model#####
##lexical information
##not using pos since it was removed from the judgment models
recalloverall.1 = glmer(Recall ~ (1|Partno) + Judgment + 
                         Judged.Value2 + LogSub.1 + LogSub.2 +
                          Length.1 + Length.2 ,
                       data = combined,
                       family = binomial,
                       control = glmerControl(optimizer = "bobyqa"),
                       nAGQ = 0)
summary(recalloverall.1)

##rated properties
recalloverall.2 = glmer(Recall ~ (1|Partno) +  Judgment + Judged.Value2 + 
                          LogSub.1 + LogSub.2 + Length.1 + Length.2 + 
                          AOA.1 +  AOA.2 +  Familiarity.1 + Familiarity.2 +  
                          Valence.1 +  Valence.2 + 
                       Imageability.1 +  Imageability.2 +  QCON.1 +  QCON.2,
                       data = combined,
                       family = binomial,
                       control = glmerControl(optimizer = "bobyqa"),
                       nAGQ = 0) ##model fails to converge when set to 1
summary(recalloverall.2) ##I don't get the error when set to 0

##network connections
recalloverall.3 = glmer(Recall ~ (1|Partno) + Judgment + Judged.Value2 + 
                          LogSub.1 + LogSub.2 + Length.1 + Length.2 + # POS.1 + POS.2,
                       AOA.1 +  AOA.2 +  Familiarity.1 +
                       Familiarity.2 +  Valence.1 +  Valence.2 + 
                       Imageability.1 +  Imageability.2 +  QCON.1 +  QCON.2 +
                       QSS.1 + TSS.2 + FSS.1 + FSS.2 + 
                       COSC.1 + COSC.2 + Ortho.1 + Ortho.2 + Phono.1 + Phono.2,
                       data = combined,
                       family = binomial,
                       control = glmerControl(optimizer = "bobyqa"),
                       nAGQ = 0)
summary(recalloverall.3)

##network norms
recalloverall.4 = glmer(Recall ~ (1|Partno) +  Judgment + Judged.Value2 + 
                          LogSub.1 + LogSub.2 + Length.1 + Length.2 + # POS.1 + POS.2,
                       AOA.1 +  AOA.2 +  Familiarity.1 +
                       Familiarity.2 +  Valence.1 +  Valence.2 + 
                       Imageability.1 +  Imageability.2 +  QCON.1 +  QCON.2 +
                       QSS.1 + TSS.2 + FSS.1 + FSS.2 + 
                       COSC.1 + COSC.2 + Ortho.1 + Ortho.2 + Phono.1 + Phono.2 +
                       ZFSG * ZLSA * ZCOS,
                       data = combined,
                       family = binomial,
                       control = glmerControl(optimizer = "bobyqa"),
                       nAGQ = 0)
summary(recalloverall.4) ##three way interaction is significant

####R-Squared for sw recall####
library(piecewiseSEM)
sem.model.fits(recalloverall.1)
sem.model.fits(recalloverall.2)
sem.model.fits(recalloverall.3)
sem.model.fits(recalloverall.4)

####moderation -- sw recall####
#low cosine
lowcos4 = glmer(Recall ~ (1|Partno) + Judgment + Judged.Value2 + 
                  LogSub.1 + LogSub.2 + Length.1 + Length.2 + # POS.1 + POS.2,
                       AOA.1 +  AOA.2 +  Familiarity.1 +
                       Familiarity.2 +  Valence.1 +  Valence.2 + 
                       Imageability.1 +  Imageability.2 +  QCON.1 +  QCON.2 +
                       QSS.1 + TSS.2 + FSS.1 + FSS.2 + 
                       COSC.1 + COSC.2 + Ortho.1 + Ortho.2 + Phono.1 + Phono.2 +
                 ZCOS_low * ZLSA * ZFSG,
                data = combined,
                family = binomial,
                control = glmerControl(optimizer = "bobyqa"),
                nAGQ = 0)
summary(lowcos4)

##high cosine
hicos4 = glmer(Recall ~ (1|Partno) + Judgment + Judged.Value2 + 
                 LogSub.1 + LogSub.2 + Length.1 + Length.2 + # POS.1 + POS.2,
                       AOA.1 +  AOA.2 +  Familiarity.1 +
                       Familiarity.2 +  Valence.1 +  Valence.2 + 
                       Imageability.1 +  Imageability.2 +  QCON.1 +  QCON.2 +
                       QSS.1 + TSS.2 + FSS.1 + FSS.2 + 
                       COSC.1 + COSC.2 + Ortho.1 + Ortho.2 + Phono.1 + Phono.2 + 
                 ZCOS_high * ZLSA * ZFSG,
               data = combined,
               family = binomial,
               control = glmerControl(optimizer = "bobyqa"),
               nAGQ = 0)
summary(hicos4)

##low cosine low lsa
lowcoslowlsa4 = glmer(Recall ~ (1|Partno) + Judgment + Judged.Value2 + 
                        LogSub.1 + LogSub.2 + Length.1 + Length.2 + # POS.1 + POS.2,
                       AOA.1 +  AOA.2 +  Familiarity.1 +
                       Familiarity.2 +  Valence.1 +  Valence.2 + 
                       Imageability.1 +  Imageability.2 +  QCON.1 +  QCON.2 +
                       QSS.1 + TSS.2 + FSS.1 + FSS.2 + 
                       COSC.1 + COSC.2 + Ortho.1 + Ortho.2 + Phono.1 + Phono.2 +
                        ZCOS_low * ZLSA_low * ZFSG,
                      data = combined,
                      family = binomial,
                      control = glmerControl(optimizer = "bobyqa"),
                      nAGQ = 0)
summary(lowcoslowlsa4)

##low cosine high lsa
lowcoshighlsa4 = glmer(Recall ~ (1|Partno) + Judgment + Judged.Value2 + 
                         LogSub.1 + LogSub.2 + Length.1 + Length.2 + # POS.1 + POS.2,
                       AOA.1 +  AOA.2 +  Familiarity.1 +
                       Familiarity.2 +  Valence.1 +  Valence.2 + 
                       Imageability.1 +  Imageability.2 +  QCON.1 +  QCON.2 +
                       QSS.1 + TSS.2 + FSS.1 + FSS.2 + 
                       COSC.1 + COSC.2 + Ortho.1 + Ortho.2 + Phono.1 + Phono.2 +
                         ZCOS_low * ZLSA_high * ZFSG,
                       data = combined,
                       family = binomial,
                       control = glmerControl(optimizer = "bobyqa"),
                       nAGQ = 0)
summary(lowcoshighlsa4)

##high low
highcoslowlsa4 = glmer(Recall ~ (1|Partno) + Judgment + Judged.Value2 + 
                         LogSub.1 + LogSub.2 + Length.1 + Length.2 + # POS.1 + POS.2,
                       AOA.1 +  AOA.2 +  Familiarity.1 +
                       Familiarity.2 +  Valence.1 +  Valence.2 + 
                       Imageability.1 +  Imageability.2 +  QCON.1 +  QCON.2 +
                       QSS.1 + TSS.2 + FSS.1 + FSS.2 + 
                       COSC.1 + COSC.2 + Ortho.1 + Ortho.2 + Phono.1 + Phono.2 +
                         ZCOS_high * ZLSA_low * ZFSG,
                       data = combined,
                       family = binomial,
                       control = glmerControl(optimizer = "bobyqa"),
                       nAGQ = 0)
summary(highcoslowlsa4)

##high high
highcoshighlsa4 = glmer(Recall ~ (1|Partno) + Judgment + Judged.Value2 + 
                          LogSub.1 + LogSub.2 + Length.1 + Length.2 + # POS.1 + POS.2,
                       AOA.1 +  AOA.2 +  Familiarity.1 +
                       Familiarity.2 +  Valence.1 +  Valence.2 + 
                       Imageability.1 +  Imageability.2 +  QCON.1 +  QCON.2 +
                       QSS.1 + TSS.2 + FSS.1 + FSS.2 + 
                       COSC.1 + COSC.2 + Ortho.1 + Ortho.2 + Phono.1 + Phono.2 +
                          ZCOS_high * ZLSA_high * ZFSG,
                        data = combined,
                        family = binomial,
                        control = glmerControl(optimizer = "bobyqa"),
                        nAGQ = 0)
summary(highcoshighlsa4)

##avg low
avgcoslowlsa4 = glmer(Recall ~ (1|Partno) + Judgment + Judged.Value2 + 
                        LogSub.1 + LogSub.2 + Length.1 + Length.2 + # POS.1 + POS.2,
                       AOA.1 +  AOA.2 +  Familiarity.1 +
                       Familiarity.2 +  Valence.1 +  Valence.2 + 
                       Imageability.1 +  Imageability.2 +  QCON.1 +  QCON.2 +
                       QSS.1 + TSS.2 + FSS.1 + FSS.2 + 
                       COSC.1 + COSC.2 + Ortho.1 + Ortho.2 + Phono.1 + Phono.2 +
                        ZCOS * ZLSA_low * ZFSG,
                      data = combined,
                      family = binomial,
                      control = glmerControl(optimizer = "bobyqa"),
                      nAGQ = 0)
summary(avgcoslowlsa4)

##avg high
avgcoshighlsa4 = glmer(Recall ~ (1|Partno) +  Judgment +Judged.Value2 + 
                         LogSub.1 + LogSub.2 + Length.1 + Length.2 + # POS.1 + POS.2,
                       AOA.1 +  AOA.2 +  Familiarity.1 +
                       Familiarity.2 +  Valence.1 +  Valence.2 + 
                       Imageability.1 +  Imageability.2 +  QCON.1 +  QCON.2 +
                       QSS.1 + TSS.2 + FSS.1 + FSS.2 + 
                       COSC.1 + COSC.2 + Ortho.1 + Ortho.2 + Phono.1 + Phono.2 +
                         ZCOS * ZLSA_high * ZFSG,
                       data = combined,
                       family = binomial,
                       control = glmerControl(optimizer = "bobyqa"),
                       nAGQ = 0) 
summary(avgcoshighlsa4)
```

```{r sw.recall.effects.table, include=FALSE}
####main effects for sw recall####
swrecall = rbind(
  cbind(rep("Step 1", nrow(coef(summary(recalloverall.1)))), coef(summary(recalloverall.1))),
cbind(rep("Step 2", nrow(coef(summary(recalloverall.2)))), coef(summary(recalloverall.2))),
cbind(rep("Step 3", nrow(coef(summary(recalloverall.3)))), coef(summary(recalloverall.3))),
cbind(rep("Step 4", nrow(coef(summary(recalloverall.4)))), coef(summary(recalloverall.4))),
cbind(rep("Low COS Low LSA", nrow(coef(summary(lowcoslowlsa4))[29:35, ])), coef(summary(lowcoslowlsa4))[29:35, ]),
cbind(rep("Low COS Average LSA", nrow(coef(summary(lowcos4))[29:35, ])), coef(summary(lowcos4))[29:35, ]),
cbind(rep("Low COS High LSA", nrow(coef(summary(lowcoshighlsa4))[29:35, ])), coef(summary(lowcoshighlsa4))[29:35, ]),
cbind(rep("Average COS Low LSA", nrow(coef(summary(avgcoslowlsa4))[29:35, ])), coef(summary(avgcoslowlsa4))[29:35, ]),
cbind(rep("Average COS High LSA", nrow(coef(summary(avgcoshighlsa4))[29:35, ])), coef(summary(avgcoshighlsa4))[29:35, ]),
cbind(rep("High COS Low LSA", nrow(coef(summary(highcoslowlsa4))[29:35, ])), coef(summary(highcoslowlsa4))[29:35, ]),
cbind(rep("High COS Average LSA", nrow(coef(summary(hicos4))[29:35, ])), coef(summary(hicos4))[29:35, ]),
cbind(rep("High COS High LSA", nrow(coef(summary(highcoshighlsa4))[29:35, ])), coef(summary(highcoshighlsa4))[29:35, ])
)

write.csv(swrecall, "single word recall.csv", row.names = T)

```

EDIT THIS SECTION

Finally, the previous set of analyses was repeated using recall as the dependent variable. A multilevel logistic regression was used, and the hierarchical design used to investigate judgments was mimicked. In addition to controlling for the type of judgments being elicited, these models also controlled for participant judgment ratings. Model steps corresponded to those used for investigating judgments. Marginal and conditional R2 values were calculated using the piecewiseSEM package in R (Lefcheck, 2016).  Lexical properties were entered into the first step (R2m = .026, R2c = .282), step two added rated word properties words (R2m = .052, R2c = .331), step three added in neighborhood connections (R2m = .062, R2c = .340), and the step four added network norms and the three-way interaction between (R2m = .082, R2c = .363) 
As with the judgment analysis, several single word norms appeared to balance out one another across cue and target items. For example, frequency (cue b = -.258, p < .001; target b = .082, p = .006), length (cue b = .138; p < .001, target b = -.047. p < .001), and feature set size. (cue b = -.012, p < .001; target b = .015, p < .001) all displayed relationships of this nature. When examining the fourth step, FSG was the strongest overall predictor of recall (b = 1.866, p < .001), and a significant three-way interaction was detected between FSG, COS, and LSA. See Table 13 for a complete list of main effects and interaction findings.

Finally, simple slopes were calculated using the same process utilized in the previous analyses to examine the three-way interaction between network norms when predicting recall. Replicating findings from the first section of Experiment Two, FSG and LSA were competitive at high COS and complimentary at low COS. Once again, this stands as a partial replication of findings from Experiment One. As with the initial replication model that did not include single word norms, the interaction present in this model is in the opposite direction as the one found in Experiment One. However, as seen with judgments, the interactive effects continued to be found even when controlling for other lexical variables. In the final section of Experiment Two, FSG and LSA were competitive at high COS and complimentary at low COS. Once again, this stands as a partial replication of findings from Experiment One.  Figure 9 illustrates these findings. 

```{r sw recall graphs}
####Recall graphs####
##low cos
plot10 = ggplot(noout, aes(x = ZCOS_low, y = Recall)) +
  labs(x = "ZFSG", y = "Recall") +
  scale_size_continuous(guide = FALSE) +
  geom_abline(aes(intercept = 6.149, slope = 2.020, linetype = "-1SD ZLSA")) +
  geom_abline(aes(intercept = 6.043, slope = 2.221, linetype = "Average ZLSA")) +
  geom_abline(aes(intercept = 5.940, slope = 2.507, linetype = "+1SD ZLSA")) +
  scale_linetype_manual(values = c("dotted", "dashed", "solid"),
                        breaks = c("-1SD ZLSA", "Average ZLSA", "+1SD ZLSA"),
                        name = "Simple Slope") +
  coord_cartesian(xlim = c(-.20, .60), ylim = c(6.1, 7)) +
  geom_vline(xintercept = -.30) +
  geom_hline(yintercept = 0) +
  cleanup + 
  labs(title="Low ZCOS") ##these intercepts are crazy high

##avg cos
plot11 = ggplot(noout, aes(x = ZCOS_low, y = Recall)) +
  labs(x = "ZFSG", y = "Recall") +
  scale_size_continuous(guide = FALSE) +
  geom_abline(aes(intercept = 6.025, slope = 2.298, linetype = "-1SD ZLSA")) +
  geom_abline(aes(intercept = 6.127, slope = 1.866, linetype = "Average ZLSA")) +
  geom_abline(aes(intercept = 6.233, slope = 1.515, linetype = "+1SD ZLSA")) +
  scale_linetype_manual(values = c("dotted", "dashed", "solid"),
                        breaks = c("-1SD ZLSA", "Average ZLSA", "+1SD ZLSA"),
                        name = "Simple Slope") +
  coord_cartesian(xlim = c(-.20, .60), ylim = c(6.1, 7)) +
  geom_vline(xintercept = -.30) +
  geom_hline(yintercept = 0) +
  cleanup + 
  labs(title="Average ZCOS")

##high cos
plot12 = ggplot(noout, aes(x = ZCOS_low, y = Recall)) +
  labs(x = "ZFSG", y = "Recall") +
  scale_size_continuous(guide = FALSE) +
  geom_abline(aes(intercept = 5.899, slope = 2.619, linetype = "-1SD ZLSA")) +
  geom_abline(aes(intercept = 6.220, slope = 1.616, linetype = "Average ZLSA")) +
  geom_abline(aes(intercept = 6.545, slope = 0.690, linetype = "+1SD ZLSA")) +
  scale_linetype_manual(values = c("dotted", "dashed", "solid"),
                        breaks = c("-1SD ZLSA", "Average ZLSA", "+1SD ZLSA"),
                        name = "Simple Slope") +
  coord_cartesian(xlim = c(-.20, .60), ylim = c(6.1, 7)) +
  geom_vline(xintercept = -.30) +
  geom_hline(yintercept = 0) +
  cleanup + 
  labs(title="High ZCOS") 

legend = get_legend(plot10)
recall.graph.sw =  plot_grid( plot10 + theme(legend.position="none"),
                           plot11 + theme(legend.position="none"),
                           plot12 + theme(legend.position="none"),
                           legend,
                           hjust = -1,
                           nrow = 2
)
recall.graph.sw 
##graph notes:
##looking at the numbers though, these are competitive at low, complimentary at high
##Just like the pilot
```

#Discussion

This study aimed to replicate interaction findings from the first experiment, first when using a novel set of stimuli and then when controlling for single word norms. First, when attempting to replicate the original interactions using the new set of stimuli, the three-way interaction was not significant when predicting participant judgments. Although the three-way interaction was not significant, a simple slopes analysis showed that FSG and LSA strengths were competitive with another at each level of semantic overlap. When extending this initial replication to predict recall, a significant three-way interaction was detected between the network norms. However, this interaction was in the opposite direction as the original findings from the first experiment, as FSG and LSA strength were found to be complimentary at low levels of semantics and became increasingly competitive at higher levels.

Similar trends were then found when attempting to replicate these interactions while controlling for the single word norms. No significant three-way interaction was detected when predicting judgment scores. Simple slopes analyses showed that increasing thematic overlap between pairs decreased the predictiveness of FSG at all levels of semantic overlap (i.e., competition at all levels). When recall was examined as the dependent variable of interest, the three-way interaction between network norms was significant. Again, the direction of this interaction was opposite to that found in Experiment One, which was consistent with the previous interaction. Simple slopes analyses revealed associative and thematic overlap were complimentary to one another at low levels of semantic overlap and became increasingly competitive as semantic overlap increased. Overall, this set of replication analyses were only partially successful, which result may be due to several limitations with the available normed databases used to select the stimuli. This is discussed in further detail at the end of this section. [EDIT THIS]

THREE TIERED HYPOTHESIS HERE

How then does this hypothesis lend itself towards the broader context of psycholinguistic research? One application of this hypothesis may be models of word recognition. One popular model is Seidenberg and McClelland (1989) "triangle model", and several variations of this model have been proposed and tested (see Harley, 2008 for a review). This model recognizes speech and reading based upon the orthography, phonology, and meaning of words. Each of these three word properties are linked in such a way that orthography is linked to phonology, phonology is linked with meaning, and meaning is linked to orthography (forming a triangle). The pathways between word properties are bidirectional, allowing for feedback between connections. Clearly, these facets are important to consider, as this study indicated that many of the phonological and orthographic variables were significantly related to judgments and recall. The bidirectional pathways may explain why cue and targets have balanced contributions to judgments and recall, as each contributes a small component to the final output from the participant. As both cue and target are activated in memory, the networks for these concepts are also activated, and each appears to be correspondingly weighted, potentially indicating that the focus of attention was spread across cue and target and these were weighted evenly.

Whereas the original version of this model focused almost exclusively on the link between orthography and phonology, Harm and Seidenberg (2004) developed a version which included a focus on semantics, with word meaning being based on input from the orthography and phonology components of the model. The results from this study indicated that associations and thematics should also be defined more clearly, rather than all incorporated into a semantic network (Maki & Buchanan, 2008). Future studies could examine how these networks and connections separate, to further distinguish how they are structured in memory. This set of experiments indicated that the relation between these values, when activated by judgment and memory processes, was often competitive.

EXPAND ON THIS

This finding may indicate separate networks that compete for attention when completing cognitive tasks. However, these findings may also support a race style model, as often described when studying reading. Each separate connection may be activated in parallel, but the weight given to each component will depend on the strength of activation of competing information. Ultimately, further studies will be needed to explore the interconnections between the semantic, thematic, and associative networks.

FIX CITATIONS

\newpage

# References

\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

